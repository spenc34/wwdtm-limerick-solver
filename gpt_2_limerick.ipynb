{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2-limerick.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMt5NM52ihotAsuIBjcn93g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bradenwatkins/wwdtm-limerick-solver/blob/master/gpt_2_limerick.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OebEtEX4p9ec",
        "colab_type": "code",
        "outputId": "f2618e7b-aedd-411e-8d29-6f97dbeff068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ny9_U1UqKNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "d3f9cfd0-c3bc-481c-a649-69b14d1af9b3"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlXybGvwqPD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7dqhR0qSC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n",
        "# get random token ID\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    \n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz8Oou5buFMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LimerickDataset(Dataset):\n",
        "  def __init__(self, path=\"limericks.txt\"):\n",
        "    super().__init__()\n",
        "\n",
        "    self.limericks = []\n",
        "    self.EOT = \"<|endoftext|>\"\n",
        "\n",
        "    with open(path) as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\"\\n\")\n",
        "      limerick, skip_count = [], 0\n",
        "      for row in csv_reader:\n",
        "        if len(row) == 0:\n",
        "          limerick.append(self.EOT)\n",
        "          self.limericks.append(\"\\n\".join(limerick))\n",
        "          limerick, skip_count = [], 0\n",
        "        elif skip_count < 2:\n",
        "          skip_count += 1\n",
        "        else:\n",
        "          limerick.append(\" \".join(row))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.limericks)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return self.limericks[item]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzTic-9tqgoP",
        "colab_type": "code",
        "outputId": "7eaa21c0-635d-4aba-f1a3-79c0d794ba74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 3e-5\n",
        "WARMUP_STEPS = 5000\n",
        "TRAINING_STEPS = 5000\n",
        "MAX_SEQ_LEN = 400\n",
        "\n",
        "dataset = LimerickDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, TRAINING_STEPS)\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_limerick_tens = None\n",
        "work_limerick_tens = None\n",
        "models_folder = \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "  os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Epoch {epoch}\")\n",
        "  for idx, limerick in enumerate(dataloader):\n",
        "    limerick_tens = torch.tensor(tokenizer.encode(limerick[0])).unsqueeze(0).to(device)\n",
        "    #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "    if limerick_tens.size()[1] > MAX_SEQ_LEN:\n",
        "      continue\n",
        "        \n",
        "    #The first limerick sequence in the sequence\n",
        "    if not torch.is_tensor(tmp_limerick_tens):\n",
        "      tmp_limerick_tens = limerick_tens\n",
        "      continue\n",
        "    else:\n",
        "      #The next limerick does not fit in so we process the sequence and leave the last limerick \n",
        "      #as the start for next sequence \n",
        "      if tmp_limerick_tens.size()[1] + limerick_tens.size()[1] > MAX_SEQ_LEN:\n",
        "        work_limerick_tens = tmp_limerick_tens\n",
        "        tmp_limerick_tens = limerick_tens\n",
        "      else:\n",
        "        #Add the limerick to sequence, continue and try to add more\n",
        "        tmp_limerick_tens = torch.cat([tmp_limerick_tens, limerick_tens[:,1:]], dim=1)\n",
        "        continue\n",
        "  \n",
        "  outputs = model(work_limerick_tens, labels=work_limerick_tens)\n",
        "  loss, logits = outputs[:2]                        \n",
        "  loss.backward()  \n",
        "  sum_loss += loss.detach().data\n",
        "  proc_seq_count += 1\n",
        "  if proc_seq_count == BATCH_SIZE:\n",
        "    proc_seq_count = 0    \n",
        "    batch_count += 1\n",
        "    optimizer.step()\n",
        "    scheduler.step() \n",
        "    optimizer.zero_grad()\n",
        "    model.zero_grad()\n",
        "\n",
        "if batch_count == 100:\n",
        "    print(f\"sum loss {sum_loss}\")\n",
        "    batch_count = 0\n",
        "    sum_loss = 0.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C4UqQNqaw_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "67ad507a-05cc-487a-8dbb-5a4a2f40537f"
      },
      "source": [
        "count = 0\n",
        "with torch.no_grad():\n",
        "  for idx, limerick in enumerate(dataloader):\n",
        "      count += 1\n",
        "      if count == 5:\n",
        "        break\n",
        "      prompt = \"\\n\".join(limerick[0].split(\"\\n\")[:-2])\n",
        "      cur_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
        "\n",
        "      non_words = set(tokenizer.encode(\"\\n,.\"))\n",
        "      next_token_id = list(non_words)[0]\n",
        "      while next_token_id in non_words:\n",
        "        outputs = model(cur_ids, labels=cur_ids)\n",
        "        loss, logits = outputs[:2]\n",
        "        softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "        next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=3)\n",
        "\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
        "\n",
        "      output = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "      print(limerick[0])\n",
        "      print()\n",
        "      print(tokenizer.decode(output))\n",
        "      print(\"======================================\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rod Stewart's not drinking Champagne yet.\n",
            "He's rocked out and worked up a drained sweat.\n",
            "He's back in his room\n",
            "with toot, toot and zoom, zoom\n",
            "because he tours with his big model\n",
            "Train set\n",
            "<|endoftext|>\n",
            "\n",
            "Rod Stewart's not drinking Champagne yet.\n",
            "He's rocked out and worked up a drained sweat.\n",
            "He's back in his room\n",
            "with toot, toot and zoom, zoom\n",
            "because he tours with his big model.\n",
            "======================================\n",
            "When signs of decay don't appear,\n",
            "the doctor will never be near.\n",
            "Three-sixty-five days\n",
            "he is keeping away\n",
            "because our apples stay crisp a whole\n",
            "Year\n",
            "<|endoftext|>\n",
            "\n",
            "When signs of decay don't appear,\n",
            "the doctor will never be near.\n",
            "Three-sixty-five days\n",
            "he is keeping away\n",
            "because our apples stay crisp a whole year\n",
            "======================================\n",
            "Playing video games is my plan.\n",
            "There's no time for a plate, pot or pan.\n",
            "My Christmas meal prop\n",
            "is an easy pop-top.\n",
            "There's three courses all packed in one\n",
            "Can\n",
            "<|endoftext|>\n",
            "\n",
            "Playing video games is my plan.\n",
            "There's no time for a plate, pot or pan.\n",
            "My Christmas meal prop\n",
            "is an easy pop-top.\n",
            "There's three courses all packed in one.\n",
            "======================================\n",
            "He's not a good RPG shooter,\n",
            "nor much of a coin and ring looter.\n",
            "My kid is quite lame\n",
            "at video games,\n",
            "so we've hired the poor guy a\n",
            "Tutor\n",
            "<|endoftext|>\n",
            "\n",
            "He's not a good RPG shooter,\n",
            "nor much of a coin and ring looter.\n",
            "My kid is quite lame\n",
            "at video games,\n",
            "so we've hired the poor guy a little\n",
            "======================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}