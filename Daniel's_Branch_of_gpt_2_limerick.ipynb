{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Daniel's Branch of gpt-2-limerick.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OebEtEX4p9ec",
        "colab_type": "code",
        "outputId": "46fcd4fe-68cd-4ce3-89f4-e156c4e30d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!pip3 install transformers\n",
        "!pip3 install tqdm\n",
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import cmudict, wordnet\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize\n",
        "!pip install pronouncing\n",
        "import pronouncing"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.33)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.33)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Requirement already satisfied: pronouncing in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: cmudict>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pronouncing) (0.4.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ny9_U1UqKNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import json\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlXybGvwqPD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7dqhR0qSC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def intersection(lst1, lst2): \n",
        "    return list(set(lst1) & set(lst2)) \n",
        "\n",
        "# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n",
        "# get random token ID\n",
        "def choose_from_top(probs, rhyme_list, n=5):\n",
        "    arr = []\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    for i in ind:\n",
        "      arr.append(tokenizer.decode(torch.ones((1,1)).long().to(device) * i))\n",
        "    # print(arr)\n",
        "    # print(rhyme_list)\n",
        "    inter = intersection(arr, rhyme_list)\n",
        "    try:\n",
        "      return inter[-1]\n",
        "    except:\n",
        "      try:\n",
        "        return rhyme_list[-1]\n",
        "      except:\n",
        "        top_prob = probs[ind]\n",
        "        top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "        \n",
        "        choice = np.random.choice(n, 1, p = top_prob)\n",
        "        token_id = ind[choice][0]\n",
        "        return tokenizer.decode(torch.ones((1,1)).long().to(device) * int(token_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz8Oou5buFMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LimerickDataset(Dataset):\n",
        "  def __init__(self, train=True):\n",
        "    super().__init__()\n",
        "    self.train = train\n",
        "    self.limericks = []\n",
        "    self.EOT = \"<|endoftext|>\"\n",
        "\n",
        "  def load(self, path):\n",
        "    if path.endswith(\".txt\"):\n",
        "      self._load_csv(path)\n",
        "    if path.endswith(\".json\"):\n",
        "      self._load_json(path)\n",
        "    return self\n",
        "\n",
        "  def _load_json(self, path):\n",
        "    with open(path) as json_file:\n",
        "      json_reader = json.load(json_file)\n",
        "      for limerick in json_reader:\n",
        "        limerick.append(self.EOT)\n",
        "        self.limericks.append(\"\\n\".join(limerick))\n",
        "\n",
        "  def _load_csv(self, path):\n",
        "    with open(path) as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=\"\\n\")\n",
        "      limerick, skip_count = [], 0\n",
        "      for row in csv_reader:\n",
        "        if len(row) == 0:\n",
        "          if self.train:\n",
        "            limerick[-2] = limerick[-2] + limerick[-1]\n",
        "            limerick.pop()\n",
        "          limerick.append(self.EOT)\n",
        "          self.limericks.append(\"\\n\".join(limerick))\n",
        "          limerick, skip_count = [], 0\n",
        "        elif skip_count < 2:\n",
        "          skip_count += 1\n",
        "        else:\n",
        "          limerick.append(\" \".join(row))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.limericks)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return self.limericks[item]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzTic-9tqgoP",
        "colab_type": "code",
        "outputId": "68f48c48-f6f7-4343-a562-ae563ecb3e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "WARMUP_STEPS = 5000\n",
        "TRAINING_STEPS = 5000\n",
        "MAX_SEQ_LEN = 400\n",
        "\n",
        "train_dataset = LimerickDataset().load(\"limericks.json\").load(\"limericks.txt\")\n",
        "print(len(train_dataset))\n",
        "dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "model.train()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, WARMUP_STEPS, TRAINING_STEPS)\n",
        "proc_seq_count = 0\n",
        "sum_loss = 0.0\n",
        "batch_count = 0\n",
        "\n",
        "tmp_limerick_tens = None\n",
        "work_limerick_tens = None\n",
        "models_folder = \"trained_models\"\n",
        "if not os.path.exists(models_folder):\n",
        "  os.mkdir(models_folder)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"Epoch {epoch + 1}\")\n",
        "  for idx, limerick in enumerate(dataloader):\n",
        "    limerick_tens = torch.tensor(tokenizer.encode(limerick[0])).unsqueeze(0).to(device)\n",
        "    #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
        "    if limerick_tens.size()[1] > MAX_SEQ_LEN:\n",
        "      continue\n",
        "        \n",
        "    #The first limerick sequence in the sequence\n",
        "    if not torch.is_tensor(tmp_limerick_tens):\n",
        "      tmp_limerick_tens = limerick_tens\n",
        "      continue\n",
        "    else:\n",
        "      #The next limerick does not fit in so we process the sequence and leave the last limerick \n",
        "      #as the start for next sequence \n",
        "      if tmp_limerick_tens.size()[1] + limerick_tens.size()[1] > MAX_SEQ_LEN:\n",
        "        work_limerick_tens = tmp_limerick_tens\n",
        "        tmp_limerick_tens = limerick_tens\n",
        "      else:\n",
        "        #Add the limerick to sequence, continue and try to add more\n",
        "        tmp_limerick_tens = torch.cat([tmp_limerick_tens, limerick_tens[:,1:]], dim=1)\n",
        "        continue\n",
        "  \n",
        "  outputs = model(work_limerick_tens, labels=work_limerick_tens)\n",
        "  loss, logits = outputs[:2]                        \n",
        "  loss.backward()  \n",
        "  sum_loss += loss.detach().data\n",
        "  proc_seq_count += 1\n",
        "  if proc_seq_count == BATCH_SIZE:\n",
        "    proc_seq_count = 0    \n",
        "    batch_count += 1\n",
        "    optimizer.step()\n",
        "    scheduler.step() \n",
        "    optimizer.zero_grad()\n",
        "    model.zero_grad()\n",
        "\n",
        "if batch_count == 100:\n",
        "    print(f\"sum loss {sum_loss}\")\n",
        "    batch_count = 0\n",
        "    sum_loss = 0.0"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47307\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vopv2vNqlxOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cmp(p, wt):\n",
        "  pt = set([s.pos() for s in wordnet.synsets(wt)])\n",
        "  # print(wt, pt)\n",
        "  return len(pt & p) > 0 or len(p) == 0 or len(pt) == 0\n",
        " \n",
        "def rhymes(s):\n",
        "  try:\n",
        "    (w, l, p, pos) = s[0]\n",
        "    try:\n",
        "      if pos[0] == 'N':\n",
        "        pos = {'n'}\n",
        "      elif pos[0] == 'V':\n",
        "        pos = {'v'}\n",
        "      elif pos[0:2] == 'RB' or pos == 'WRB':\n",
        "        pos = {'r'}\n",
        "      elif pos[0] == 'J':\n",
        "        pos = {'a'}\n",
        "      else:\n",
        "        pos = set([s.pos() for s in wordnet.synsets(w)])\n",
        "      # print(w, pos)\n",
        "      filtered = [' '+wt for (wt, pt) in cmudict.entries() \\\n",
        "                  if ((l == len(pt) \\\n",
        "                  and p[-2:] == pt[-2:]) \\\n",
        "                  or (l == len(pt)-1 \\\n",
        "                  and p == pt[-2:]) \\\n",
        "                  or (l == len(pt)+1 \\\n",
        "                  and pt == p[-2:]) \\\n",
        "                  or (l == 2 \\\n",
        "                  and len(pt) == 2 \\\n",
        "                  and p[-1:] == pt[-1:])) \\\n",
        "                  and (nltk.distance.edit_distance(w, wt) > 2 \\\n",
        "                  or not w[0:2] == wt[0:2]) \\\n",
        "                  and cmp(pos, wt) \\\n",
        "                  and len(nltk.corpus.wordnet.synsets(wt)) > 0]\n",
        "      return filtered\n",
        "    except:\n",
        "      return [w]\n",
        "  except:\n",
        "    return []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C4UqQNqaw_T",
        "colab_type": "code",
        "outputId": "48e1d641-3118-4a0b-8fb0-92e322ffce7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "count = 5\n",
        "test_dataset = LimerickDataset(train=False).load(\"limericks.txt\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "print(len(test_dataset))\n",
        "with torch.no_grad():\n",
        "  for idx, limerick in enumerate(test_dataloader):\n",
        "      if not count:\n",
        "        break\n",
        "      count -= 1\n",
        "      prompt = \"\\n\".join(limerick[0].split(\"\\n\")[:-2])\n",
        "      sample = limerick[0].split(\"\\n\")[:-2]\n",
        "      tokens = [wordpunct_tokenize(s) for s in sample]\n",
        "      tagged = [nltk.pos_tag(t) for t in tokens]\n",
        "      punct = set(['.', ',', '!', ':', ';', '-'])\n",
        "      filtered = [ [w for w in sentence if w[0] not in punct ] for sentence in tagged]\n",
        "      last = [ sentence[len(sentence) - 1] for sentence in filtered][0:2]\n",
        "      # print(last)\n",
        "      syllables = [[(word, len(pron), pron, w[1]) for (word, pron) in cmudict.entries() if word == w[0]] for w in last]\n",
        "      rhyme_list = [rhymes(s) for s in syllables]\n",
        "      rhyme_list = np.concatenate(rhyme_list)\n",
        "\n",
        "      cur_ids = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
        "\n",
        "      non_words = set(tokenizer.encode(\"\\n,.\"))\n",
        "      next_token_id = list(non_words)[0]\n",
        "      # while next_token_id in non_words:\n",
        "      outputs = model(cur_ids, labels=cur_ids)\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token = choose_from_top(softmax_logits.to('cpu').numpy(), rhyme_list, n=50000)\n",
        "\n",
        "      cur_ids = prompt + next_token # Add the last word to the running sequence\n",
        "\n",
        "      # output = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "      print(limerick[0])\n",
        "      print()\n",
        "      print(cur_ids)\n",
        "      print(\"======================================\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "He's not a good RPG shooter,\n",
            "nor much of a coin and ring looter.\n",
            "My kid is quite lame\n",
            "at video games,\n",
            "so we've hired the poor guy a\n",
            "Tutor\n",
            "<|endoftext|>\n",
            "\n",
            "He's not a good RPG shooter,\n",
            "nor much of a coin and ring looter.\n",
            "My kid is quite lame\n",
            "at video games,\n",
            "so we've hired the poor guy a writer\n",
            "======================================\n",
            "When the world is in steady decline,\n",
            "quit pretending that everything's fine.\n",
            "You'll be easing your pain\n",
            "if you moan and complain.\n",
            "It's much better for you if you\n",
            "Whine\n",
            "<|endoftext|>\n",
            "\n",
            "When the world is in steady decline,\n",
            "quit pretending that everything's fine.\n",
            "You'll be easing your pain\n",
            "if you moan and complain.\n",
            "It's much better for you if you combine\n",
            "======================================\n",
            "They can see how I twitch when I'm dreaming.\n",
            "I sure hope that I don't wake up screaming.\n",
            "At a dollar a night,\n",
            "the hotel is priced right\n",
            "'cause my room's on a feed that's live\n",
            "Streaming\n",
            "<|endoftext|>\n",
            "\n",
            "They can see how I twitch when I'm dreaming.\n",
            "I sure hope that I don't wake up screaming.\n",
            "At a dollar a night,\n",
            "the hotel is priced right\n",
            "'cause my room's on a feed that's live sterling\n",
            "======================================\n",
            "Over every new tech, we are lords.\n",
            "So why is it that everyone hoards?\n",
            "We're awaiting the hour\n",
            "our tape decks need power.\n",
            "We all have a box of old\n",
            "Cords\n",
            "<|endoftext|>\n",
            "\n",
            "Over every new tech, we are lords.\n",
            "So why is it that everyone hoards?\n",
            "We're awaiting the hour\n",
            "our tape decks need power.\n",
            "We all have a box of old bends\n",
            "======================================\n",
            "That duct tape banana, I hate it.\n",
            "More than 100 grand is what I rate it.\n",
            "Oh, sure it is art.\n",
            "So I'm playing my part.\n",
            "Because I was hungry, I\n",
            "Ate it\n",
            "<|endoftext|>\n",
            "\n",
            "That duct tape banana, I hate it.\n",
            "More than 100 grand is what I rate it.\n",
            "Oh, sure it is art.\n",
            "So I'm playing my part.\n",
            "Because I was hungry, I mit\n",
            "======================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}